{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc961547-20f1-4eaf-89f6-9a378cb831e2",
   "metadata": {},
   "source": [
    "## Converter of oscilloscope data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d91fb-6871-4607-bcd3-88765bc2993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import csv\n",
    "import pyarrow as pa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0fbe2-ee78-446c-b817-59404e2c80e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### converter for 3 channels oscilloscope.csv -> cleaned.csv //have to add correct time-step resolution\n",
    "RESOLUTION = 2e-11\n",
    "BUFFER_SIZE = 50_000  # number of rows to buffer before writing\n",
    "\n",
    "def convert3channels_csv_to_csv_and_parquet(in_path, out_csv_path, out_parquet_path):\n",
    "    buffer = []\n",
    "    event_id = -1\n",
    "    t0 = None\n",
    "    idx = 0\n",
    "\n",
    "    parquet_writer = None\n",
    "\n",
    "    with open(in_path) as fin, open(out_csv_path, \"w\") as fout:\n",
    "        # write CSV header\n",
    "        fout.write(\"event_id,timestamp,Ch_1,Ch_2,Ch_3\\n\")\n",
    "\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            parts = line.split(\";\")\n",
    "\n",
    "            # new event\n",
    "            if len(parts) == 1:\n",
    "                event_id += 1\n",
    "                t0 = float(parts[0])\n",
    "                idx = 0\n",
    "                continue\n",
    "\n",
    "            # data row\n",
    "            if len(parts) == 3:\n",
    "                timestamp = t0 + idx * RESOLUTION\n",
    "                idx += 1\n",
    "                x, y, z = map(float, parts)\n",
    "\n",
    "                # add to buffer\n",
    "                buffer.append([event_id, timestamp, x, y, z])\n",
    "\n",
    "                # flush buffer if full\n",
    "                if len(buffer) >= BUFFER_SIZE:\n",
    "                    # write CSV\n",
    "                    for row in buffer:\n",
    "                        fout.write(f\"{row[0]},{row[1]},{row[2]},{row[3]},{row[4]}\\n\")\n",
    "\n",
    "                    # write Parquet\n",
    "                    df = pd.DataFrame(buffer, columns=[\"event_id\", \"timestamp\", \"Ch_1\", \"Ch_2\", \"Ch_3\"])\n",
    "                    table = pa.Table.from_pandas(df)\n",
    "                    if parquet_writer is None:\n",
    "                        parquet_writer = pq.ParquetWriter(out_parquet_path, table.schema, compression='snappy')\n",
    "                    parquet_writer.write_table(table)\n",
    "\n",
    "                    buffer = []\n",
    "\n",
    "                continue\n",
    "\n",
    "            raise ValueError(f\"Unexpected format: {line}\")\n",
    "\n",
    "        # flush remaining buffer\n",
    "        if buffer:\n",
    "            for row in buffer:\n",
    "                fout.write(f\"{row[0]},{row[1]},{row[2]},{row[3]},{row[4]}\\n\")\n",
    "            df = pd.DataFrame(buffer, columns=[\"event_id\", \"timestamp\", \"Ch_1\", \"Ch_2\", \"Ch_3\"])\n",
    "            table = pa.Table.from_pandas(df)\n",
    "            if parquet_writer is None:\n",
    "                parquet_writer = pq.ParquetWriter(out_parquet_path, table.schema, compression='snappy')\n",
    "            parquet_writer.write_table(table)\n",
    "\n",
    "        if parquet_writer:\n",
    "            parquet_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58de4b2-83a2-4a1c-9ce8-bdd255780ddd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### converter for 2 channels oscilloscope.csv -> cleaned.csv //have to add correct time-step resolution\n",
    "RESOLUTION = 2e-11\n",
    "BUFFER_SIZE = 50_000  # number of rows to buffer before writing\n",
    "\n",
    "def convert2channels_csv_to_csv_and_parquet(in_path, out_csv_path, out_parquet_path):\n",
    "    buffer = []\n",
    "    event_id = -1\n",
    "    t0 = None\n",
    "    idx = 0\n",
    "\n",
    "    parquet_writer = None\n",
    "\n",
    "    with open(in_path) as fin, open(out_csv_path, \"w\") as fout:\n",
    "        # write CSV header\n",
    "        fout.write(\"event_id,timestamp,Ch_1,Ch_2\\n\")\n",
    "\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            parts = line.split(\";\")\n",
    "\n",
    "            # new event\n",
    "            if len(parts) == 1:\n",
    "                event_id += 1\n",
    "                t0 = float(parts[0])\n",
    "                idx = 0\n",
    "                continue\n",
    "\n",
    "            # data row\n",
    "            if len(parts) == 2:\n",
    "                timestamp = t0 + idx * RESOLUTION\n",
    "                idx += 1\n",
    "                x, y = map(float, parts)\n",
    "\n",
    "                # add to buffer\n",
    "                buffer.append([event_id, timestamp, x, y])\n",
    "\n",
    "                # flush buffer if full\n",
    "                if len(buffer) >= BUFFER_SIZE:\n",
    "                    # write CSV\n",
    "                    for row in buffer:\n",
    "                        fout.write(f\"{row[0]},{row[1]},{row[2]},{row[3]}\\n\")\n",
    "\n",
    "                    # write Parquet\n",
    "                    df = pd.DataFrame(buffer, columns=[\"event_id\", \"timestamp\", \"Ch_1\", \"Ch_2\"])\n",
    "                    table = pa.Table.from_pandas(df)\n",
    "                    if parquet_writer is None:\n",
    "                        parquet_writer = pq.ParquetWriter(out_parquet_path, table.schema, compression='snappy')\n",
    "                    parquet_writer.write_table(table)\n",
    "\n",
    "                    buffer = []\n",
    "\n",
    "                continue\n",
    "\n",
    "            raise ValueError(f\"Unexpected format: {line}\")\n",
    "\n",
    "        # flush remaining buffer\n",
    "        if buffer:\n",
    "            for row in buffer:\n",
    "                fout.write(f\"{row[0]},{row[1]},{row[2]},{row[3]}\\n\")\n",
    "            df = pd.DataFrame(buffer, columns=[\"event_id\", \"timestamp\", \"Ch_1\", \"Ch_2\"])\n",
    "            table = pa.Table.from_pandas(df)\n",
    "            if parquet_writer is None:\n",
    "                parquet_writer = pq.ParquetWriter(out_parquet_path, table.schema, compression='snappy')\n",
    "            parquet_writer.write_table(table)\n",
    "\n",
    "        if parquet_writer:\n",
    "            parquet_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cfd643-f92f-4e3b-8315-7b608ff39768",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert2channels_csv_to_csv_and_parquet('/Users/laloscola/ExternalDrive/magic data laboratory/ratio_check_data/RatioCheck_Run11_2025-12-03_0_160508.Wfm.csv', '/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run11.csv', '/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run11.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3443aecb-20d9-4f82-bff1-4411047cb8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert2channels_csv_to_csv_and_parquet('/Users/laloscola/ExternalDrive/magic data laboratory/ratio_check_data/RatioCheck_Run12_2025-12-03_1_160822.Wfm.csv', '/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run12.csv', '/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run12.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe6f2be-b103-4e9c-8184-d6ce9a06e01b",
   "metadata": {},
   "source": [
    "## Code for Ratio and Visual Check\n",
    "$\\rightarrow$ aiming for 1/10 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d038ef-81bc-45e0-a45a-06a54572d800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import csv\n",
    "import pyarrow as pa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f5c1c-ba1d-4939-8ca2-2ef95b58f5d6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_df(data): # load data no matter if already df or file\n",
    "    # Case 1: filename \n",
    "    if isinstance(data, str):\n",
    "        df = pd.read_csv(data)   \n",
    "    \n",
    "    # Case 2: already a DataFrame \n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        df = data\n",
    "    \n",
    "    else:\n",
    "        raise TypeError(\"Input must be a filename or a pandas DataFrame.\")\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a5cee2-7e72-486e-baba-cca1af0e18e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_all_channels(df, target_event = 0):\n",
    "    # filter for target event\n",
    "    event= df[df['event_id'] == target_event] \n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(6,2))\n",
    "    plt.plot(event[\"timestamp\"], event[\"Ch_1\"], label=\"Laser Trigger\")\n",
    "    plt.plot(event[\"timestamp\"], event[\"Ch_2\"], label=\"MCP PMT Camera\")\n",
    "    plt.plot(event[\"timestamp\"], event[\"Ch_3\"], label=\"HPD\")\n",
    "    plt.xlabel(\"Timestamp (s)\")\n",
    "    plt.ylabel(\"Signal\")\n",
    "    plt.title(f\"Event {target_event}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d83b82-6691-49cd-a976-37549f1c05e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_only_camera(df, target_event = 0):\n",
    "    # filter for target event\n",
    "    event= df[df['event_id'] == target_event] \n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(6,2))\n",
    "    plt.plot(event[\"timestamp\"], event[\"Ch_2\"], label=\"MCP PMT Camera\")\n",
    "    plt.xlabel(\"Timestamp (s)\")\n",
    "    plt.ylabel(\"Signal\")\n",
    "    plt.title(f\"Event {target_event}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e412fe-a633-4189-9068-814a82f7c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interesting_value_filter(df, min_thresh = -0.01):  # filters interesting events according to thresh\n",
    "    # one grouped min over the whole DataFrame\n",
    "    mins = df.groupby('event_id')['Ch_2'].min()\n",
    "    # select only those event_ids whose min < threshold\n",
    "    interesting_ids = mins[mins < min_thresh].index.to_list()\n",
    "    return interesting_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad1712-0166-4243-b04f-b4b8d9ae0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_ratio_and_plots(data, min_thresh = -0.01, show_all_channels=False, show_only_camera=True):\n",
    "    '''Usage:\n",
    "        1. Input of the cleaned file after convert_csv_to_csv_and_parquet\n",
    "        2. Define acceptable threshold for the minima one wants to accept\n",
    "       Purpose:\n",
    "        - takes an oscilloscope dataset and returns the ratio of \n",
    "    '''\n",
    "    df = load_df(data)  # returns df if either pandas dataframe or directly from file\n",
    "    interesting_ids = interesting_value_filter(df, min_thresh)\n",
    "    \n",
    "    ratio = len(interesting_ids)/(max(df['event_id'])+1-len(interesting_ids))\n",
    "    percentage = ratio * 100\n",
    "    print(f'There are: {len(interesting_ids)} interesting events, that match the used threshold')\n",
    "    # print(f'These events are numbered: {interesting_ids}')\n",
    "    print(f'The percentage of interesting events to empty events is: {percentage:.3f}%')\n",
    "\n",
    "    if show_all_channels:\n",
    "        for i in interesting_ids:\n",
    "            plot_all_channels(df, target_event=i)\n",
    "\n",
    "    if show_only_camera:\n",
    "        for i in interesting_ids:\n",
    "            plot_only_camera(df, target_event=i)\n",
    "\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27b6bcc-c281-435c-b053-147897154760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_ratio_and_plots('/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run11.csv', min_thresh=-0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98536dc-0990-418c-ad92-ce0647cd3b22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_ratio_and_plots('/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run12.csv', min_thresh=-0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6beefd1-2b78-4204-8ef1-888cc8696db4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_ratio_and_plots('/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run10.csv', min_thresh=-0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1794949-0111-4df3-ba9a-fc33127dcc71",
   "metadata": {},
   "source": [
    "### Results: \n",
    "- oversaturation for Runs: 11,12 -> diffuser very close and light very bright -> fewer clean peaks\n",
    "    - about ~5% ratio\n",
    "- for Run 10: also about ~5% ratio but more clean peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb293ef-5e1a-4cee-8bfe-ff535108adf8",
   "metadata": {},
   "source": [
    "## Code for Jitter Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05cdcf1-6592-42ef-b7b1-e83a70546f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import csv\n",
    "import pyarrow as pa \n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ccfd17-e059-41bb-9bee-326f10352d18",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def peak_time_from_trace(t, y, height=None, prominence=None):  # calculates peak timing using find_peaks from scipy\n",
    "    peaks, props = find_peaks(y, height=height, prominence=prominence)\n",
    "    if len(peaks) == 0:\n",
    "        return None  # or raise\n",
    "    # choose the highest / most prominent peak\n",
    "    i = peaks[np.argmin(props[\"peak_heights\"])]\n",
    "    return t[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf2452d-9d25-454e-85c8-90621b9c7d0e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def cf_neg_pulse_time(t, y, frac=0.9):  # calculates the first element idx that is above the thresh \n",
    "    \"\"\"\n",
    "    Constant-fraction time pickoff. \n",
    "    Assumes a negative pulse on top of baseline-subtracted data.\n",
    "    \"\"\"\n",
    "    A = y.min()\n",
    "    if A >= 0:\n",
    "        return None\n",
    "    thr = frac * A # calculates max Amplitude A and determines the thresh based on A \n",
    "\n",
    "    idx = np.where(y <= thr)[0] # selects first index where y is above thresh -> timing based on first flank\n",
    "    if len(idx) == 0:\n",
    "        return None\n",
    "        \n",
    "    i1 = idx[0] # renaming\n",
    "    if i1 == 0:\n",
    "        return t[0]\n",
    "        \n",
    "    i0 = i1 - 1 # shifting by one index for linear interepolation\n",
    "\n",
    "    t0, t1 = t[i0], t[i1]\n",
    "    y0, y1 = y[i0], y[i1]\n",
    "    if y1 == y0:\n",
    "        return t1  # fallback\n",
    "    # linear interpolation for values that satisfy threshold\n",
    "    return t0 + (thr - y0) * (t1 - t0) / (y1 - y0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d3d18-49f0-48fd-8639-39ea1d538a4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def cf_pos_pulse_time(t, y, frac=0.7): # time pickoff via constant-fraction for one event\n",
    "    \"\"\"\n",
    "    Constant-fraction time pickoff.\n",
    "    Assumes a positive pulse on top of baseline-subtracted data.\n",
    "    \"\"\"\n",
    "    A = y.max()\n",
    "    if A <= 0:\n",
    "        return None\n",
    "    thr = frac * A\n",
    "\n",
    "    idx = np.where(y >= thr)[0]\n",
    "    if len(idx) == 0:\n",
    "        return None\n",
    "    i1 = idx[0]\n",
    "    if i1 == 0:\n",
    "        return t[0]\n",
    "    i0 = i1 - 1\n",
    "\n",
    "    t0, t1 = t[i0], t[i1]\n",
    "    y0, y1 = y[i0], y[i1]\n",
    "    if y1 == y0:\n",
    "        return t1  # fallback\n",
    "    # linear interpolation\n",
    "    return t0 + (thr - y0) * (t1 - t0) / (y1 - y0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f8a16d-cfb8-48ad-856c-df858c6b8bf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def subtract_baseline(t, y, t_pre_max=None):\n",
    "    \"\"\"\n",
    "    Subtract baseline estimated from pre-signal region.\n",
    "    If t_pre_max is None, use first 10% of the time window.\n",
    "    \"\"\"\n",
    "    if t_pre_max is None:\n",
    "        t_pre_max = t.min() + 0.1 * (t.max() - t.min())\n",
    "    mask_pre = t < t_pre_max\n",
    "    if not np.any(mask_pre):\n",
    "        baseline = np.median(y)\n",
    "    else:\n",
    "        baseline = np.median(y[mask_pre])\n",
    "    return y - baseline, baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744fc92f-11a8-43ca-b756-3f6f02c64dc4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def event_time_diff(event_df, ch_ref=\"Ch_1\", ch_test=\"Ch_2\",\n",
    "                    frac=0.9, t_pre_max=None):\n",
    "    \"\"\"\n",
    "    Compute t_test - t_ref for a single event DataFrame.\n",
    "    Returns None if timing cannot be determined.\n",
    "    \"\"\"\n",
    "    t = event_df[\"timestamp\"].to_numpy()\n",
    "\n",
    "    y_ref = event_df[ch_ref].to_numpy()\n",
    "    y_ref_bs, _ = subtract_baseline(t, y_ref, t_pre_max)\n",
    "    t_ref = cf_pos_pulse_time(t, y_ref_bs, frac=frac)\n",
    "\n",
    "    y_test = event_df[ch_test].to_numpy()\n",
    "    y_test_bs, _ = subtract_baseline(t, y_test, t_pre_max)\n",
    "    t_test = cf_neg_pulse_time(t, y_test_bs, frac=frac)\n",
    "\n",
    "    if t_ref is None or t_test is None:\n",
    "        return None\n",
    "    return t_test - t_ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fea0fe-fc02-463f-be40-1cd9ccb7333b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_jitter(df, ch_ref=\"Ch_1\", ch_test=\"Ch_2\",\n",
    "                   frac=0.5, t_pre_max=None):\n",
    "    \"\"\"\n",
    "    Loop over all event_id in df, compute time differences (test - ref),\n",
    "    and return the list of deltas plus mean and std (jitter).\n",
    "    \"\"\"\n",
    "    deltas = []\n",
    "    for eid in df[\"event_id\"].unique():\n",
    "        event = df[df[\"event_id\"] == eid].sort_values(\"timestamp\")\n",
    "        dt = event_time_diff(event, ch_ref=ch_ref, ch_test=ch_test,\n",
    "                             frac=frac, t_pre_max=t_pre_max)\n",
    "        if dt is not None:\n",
    "            deltas.append(dt)\n",
    "\n",
    "    deltas = np.array(deltas)\n",
    "    if len(deltas) == 0:\n",
    "        return deltas, None, None\n",
    "\n",
    "    mean_offset = deltas.mean()\n",
    "    jitter = deltas.std(ddof=1)  # RMS jitter\n",
    "    return deltas, mean_offset, jitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c696d8-69e4-4e21-9ffc-5c52dd4292a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_jitter_histogram(deltas, bins=200, Run_Number = None, Counts=None, percentage = None):  # plot of the jitter distribution\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.hist(deltas, bins=bins, alpha=0.8, edgecolor=\"k\")  # example: ps\n",
    "    plt.xlabel(\"Time difference (s)\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(f\"Jitter distribution of Run {Run_Number} with {Counts} acqusitions at ratio {percentage}%\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"JitterHist_Run{Run_Number}_Counts{Counts}.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_jitter_vs_event(deltas, Run_Number = None, Counts=None, percentage = None):  # if one wishes to find the deviating events\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot(np.arange(len(deltas)), deltas , \".\", ms=3)\n",
    "    plt.xlabel(\"Event index\")\n",
    "    plt.ylabel(\"Time difference (s)\")\n",
    "    plt.title(f\"Jitter vs event of Run {Run_Number} with {Counts} acqusitions at ratio {percentage}%\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"JitterVsEventHist_Run{Run_Number}_Counts{Counts}.png\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b564c4-754a-4072-ab9e-4f9fb5a3f306",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fwhm_from_hist(deltas, bins=100, range=None, frac=0.5):  ### useless for calculations\n",
    "    counts, bin_edges = np.histogram(deltas, bins=bins, range=range)\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "    max_count = counts.max()\n",
    "    if max_count == 0:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # peak center (mode) as max-count bin center\n",
    "    peak_idx = np.argmax(counts)\n",
    "    peak_center = bin_centers[peak_idx]\n",
    "\n",
    "    half_max = frac * max_count\n",
    "    above = np.where(counts >= half_max)[0]\n",
    "    if len(above) < 2:\n",
    "        return None, None, None, None, peak_center\n",
    "\n",
    "    left = bin_centers[above[0]]\n",
    "    right = bin_centers[above[-1]]\n",
    "    fwhm = right - left\n",
    "    return fwhm, left, right, half_max, peak_center\n",
    "\n",
    "def plot_jitter_histograms_with_zoom(deltas, bins=80, unit_scale=1e12, unit_label=\"ps\", zoom_factor=2.0):\n",
    "    \"\"\"\n",
    "    Plots:\n",
    "      1) Full jitter histogram with FWHM\n",
    "      2) Zoomed-in histogram around the peak (± zoom_factor * FWHM / 2)\n",
    "    \"\"\"\n",
    "    # FWHM in seconds\n",
    "    fwhm_s, left_s, right_s, half_max, peak_s = fwhm_from_hist(deltas, bins=bins)\n",
    "    if fwhm_s is None:\n",
    "        print(\"Could not determine FWHM for jitter.\")\n",
    "        return\n",
    "\n",
    "    # convert to plotting units\n",
    "    deltas_u = deltas * unit_scale\n",
    "    fwhm_u = fwhm_s * unit_scale\n",
    "    left_u = left_s * unit_scale\n",
    "    right_u = right_s * unit_scale\n",
    "    peak_u = peak_s * unit_scale\n",
    "\n",
    "    # ---------- 1) Full histogram ----------\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.hist(deltas_u, bins=bins, alpha=0.8, edgecolor=\"k\")\n",
    "    plt.xlabel(f\"Time difference ({unit_label})\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(\"Jitter distribution (full)\")\n",
    "\n",
    "    plt.axvline(left_u, color=\"red\", linestyle=\"--\", label=\"FWHM edges\")\n",
    "    plt.axvline(right_u, color=\"red\", linestyle=\"--\")\n",
    "    plt.text(0.05, 0.95,\n",
    "             f\"FWHM ≈ {fwhm_u:.2f} {unit_label}\",\n",
    "             transform=plt.gca().transAxes,\n",
    "             va=\"top\", ha=\"left\", color=\"red\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ---------- 2) Zoomed histogram ----------\n",
    "    half_width_u = (fwhm_u / 2) * zoom_factor\n",
    "    x_min = peak_u - half_width_u\n",
    "    x_max = peak_u + half_width_u\n",
    "\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.hist(deltas_u, bins=bins, alpha=0.8, edgecolor=\"k\")\n",
    "    plt.xlabel(f\"Time difference ({unit_label})\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(\"Jitter distribution (zoomed)\")\n",
    "    plt.xlim(x_min, x_max)\n",
    "\n",
    "    plt.axvline(left_u, color=\"red\", linestyle=\"--\", label=\"FWHM edges\")\n",
    "    plt.axvline(right_u, color=\"red\", linestyle=\"--\")\n",
    "    plt.text(0.05, 0.95,\n",
    "             f\"FWHM ≈ {fwhm_u:.2f} {unit_label}\",\n",
    "             transform=plt.gca().transAxes,\n",
    "             va=\"top\", ha=\"left\", color=\"red\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bd52da-b0de-4b4b-9973-e4967f81a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss(x, A, mu, sigma, C):\n",
    "    return A * np.exp(-0.5 * ((x - mu) / sigma) ** 2) + C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae5969-c944-470e-9821-f5ab2bf1f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gaussian_to_jitter(deltas, bins=50, range=None):\n",
    "    \"\"\"\n",
    "    Fit a Gaussian to the jitter histogram (in seconds).\n",
    "    Returns:\n",
    "        popt  : [A, mu, sigma, C]\n",
    "        pcov  : covariance matrix\n",
    "        bin_centers, counts : histogram data used for fit\n",
    "    \"\"\"\n",
    "    counts, bin_edges = np.histogram(deltas, bins=bins, range=range)\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "\n",
    "    if counts.max() == 0:\n",
    "        return None, None, bin_centers, counts\n",
    "\n",
    "    # initial guesses from histogram maximum\n",
    "    i_max = np.argmax(counts)\n",
    "    mu0 = bin_centers[i_max]\n",
    "    sigma0 = np.std(deltas)\n",
    "    if sigma0 == 0:\n",
    "        sigma0 = (bin_centers[-1] - bin_centers[0]) / 10.0\n",
    "    A0 = counts.max()\n",
    "    C0 = counts.min()\n",
    "\n",
    "    p0 = [A0, mu0, sigma0, C0]\n",
    "\n",
    "    try:\n",
    "        popt, pcov = curve_fit(gauss, bin_centers, counts, p0=p0)\n",
    "    except RuntimeError:\n",
    "        return None, None, bin_centers, counts\n",
    "\n",
    "    return popt, pcov, bin_centers, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2e1e5-5cb2-4414-b4b2-2aef5fb08e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_jitter_gaussian_two_views(deltas,\n",
    "                                   bins=50,\n",
    "                                   unit_scale=1e9,\n",
    "                                   unit_label=\"ns\",\n",
    "                                   zoom_factor=2.0, \n",
    "                                   Run_Number = None , Counts=None , percentage = None):\n",
    "    \"\"\"\n",
    "    Plot jitter histogram + Gaussian fit in two views:\n",
    "      1) Full x-range\n",
    "      2) Zoomed around the peak (± zoom_factor * FWHM/2)\n",
    "    deltas are in seconds; unit_scale converts to desired units for plotting.\n",
    "    \"\"\"\n",
    "    # --- Fit Gaussian on histogram in seconds ---\n",
    "    popt, pcov, bin_centers_s, counts = fit_gaussian_to_jitter(deltas, bins=bins)\n",
    "    if popt is None:\n",
    "        print(\"Gaussian fit to jitter histogram failed.\")\n",
    "        return None, None, None\n",
    "\n",
    "    A, mu_s, sigma_s, C = popt\n",
    "    sigma_s = np.abs(sigma_s)\n",
    "    fwhm_s = 2.0 * np.sqrt(2.0 * np.log(2.0)) * sigma_s  # Gaussian FWHM [web:202]\n",
    "\n",
    "    # Convert to user units\n",
    "    deltas_u = deltas * unit_scale\n",
    "    bin_centers_u = bin_centers_s * unit_scale\n",
    "    mu_u = mu_s * unit_scale\n",
    "    sigma_u = sigma_s * unit_scale\n",
    "    fwhm_u = fwhm_s * unit_scale\n",
    "\n",
    "    # x-grid for fitted curve in user units\n",
    "    x_fit_u = np.linspace(bin_centers_u.min(), bin_centers_u.max(), 1000)\n",
    "    x_fit_s = x_fit_u / unit_scale\n",
    "    y_fit = gauss(x_fit_s, *popt)\n",
    "\n",
    "    left_u = mu_u - 0.5 * fwhm_u\n",
    "    right_u = mu_u + 0.5 * fwhm_u\n",
    "\n",
    "    # ---------- 1) Full view ----------\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(deltas_u, bins=bins, alpha=0.7, edgecolor=\"k\", label=\"Data\")\n",
    "    plt.plot(x_fit_u, y_fit, \"r-\", label=\"Gaussian fit\")\n",
    "\n",
    "    plt.axvline(left_u, color=\"green\", linestyle=\"--\", label=\"FWHM\")\n",
    "    plt.axvline(right_u, color=\"green\", linestyle=\"--\")\n",
    "\n",
    "    plt.xlabel(f\"Time difference ({unit_label})\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(f\"Jitter distribution (full) of Run {Run_Number} with {Counts} acquisitions at ratio {percentage}%\")\n",
    "\n",
    "    plt.text(\n",
    "        0.05, 0.95,\n",
    "        f\"μ ≈ {mu_u:.2f} {unit_label}\\nσ ≈ {sigma_u:.2f} {unit_label}\\nFWHM ≈ {fwhm_u:.2f} {unit_label}\",\n",
    "        transform=plt.gca().transAxes,\n",
    "        va=\"top\", ha=\"left\", color=\"black\"\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"JitterFitFullView_Run{Run_Number}_Counts{Counts}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # ---------- 2) Zoomed view ----------\n",
    "    half_width_u = 0.5 * fwhm_u * zoom_factor\n",
    "    x_min = mu_u - half_width_u\n",
    "    x_max = mu_u + half_width_u\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(deltas_u, bins=bins, alpha=0.7, edgecolor=\"k\", label=\"Data\")\n",
    "    plt.plot(x_fit_u, y_fit, \"r-\", label=\"Gaussian fit\")\n",
    "\n",
    "    plt.axvline(left_u, color=\"green\", linestyle=\"--\", label=\"FWHM\")\n",
    "    plt.axvline(right_u, color=\"green\", linestyle=\"--\")\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.xlabel(f\"Time difference ({unit_label})\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(f\"Jitter distribution (zoomed) of Run {Run_Number} with {Counts} acquisitions at ratio {percentage}%\")\n",
    "\n",
    "    plt.text(\n",
    "        0.05, 0.95,\n",
    "        f\"μ ≈ {mu_u:.2f} {unit_label}\\nσ ≈ {sigma_u:.2f} {unit_label}\\nFWHM ≈ {fwhm_u:.2f} {unit_label}\",\n",
    "        transform=plt.gca().transAxes,\n",
    "        va=\"top\", ha=\"left\", color=\"black\"\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"JitterFitZoomed_Run{Run_Number}_Counts{Counts}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    return mu_s, sigma_s, fwhm_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfbbf74-8cfe-4694-88ac-d1fe6026ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_jitter_plot_and_fwhm(data, bins_fit = 50, zoom_factor= 4, Run_Number = None, Counts=None, percentage = None):\n",
    "    df = load_df(data)\n",
    "    int_ids = interesting_value_filter(df)\n",
    "\n",
    "    int_df = df[df['event_id'].isin(int_ids)]\n",
    "    deltas, mean_offset, jitter = compute_jitter(int_df)\n",
    "   \n",
    "    plot_jitter_histogram(deltas, Run_Number=Run_Number, Counts=Counts, percentage=percentage)\n",
    "    plot_jitter_vs_event(deltas, Run_Number=Run_Number, Counts=Counts, percentage=percentage)\n",
    "    plot_jitter_gaussian_two_views(deltas, bins=bins_fit, zoom_factor=zoom_factor, Run_Number=Run_Number, Counts=Counts, percentage=percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95b989-9043-4e5e-a08b-dc23f77c73bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_jitter_plot_and_fwhm('/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run10.csv', bins_fit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e657b-9ad5-4ebd-89da-0620bcaeafd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_jitter_plot_and_fwhm('/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run11.csv', bins_fit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc891f-1ea0-423d-b59e-d4ff696140ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_jitter_plot_and_fwhm('/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run12.csv', bins_fit = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab739060-8494-44e6-bf38-bad9b3d29957",
   "metadata": {},
   "source": [
    "## Integration for Amplitude-Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67ffc3-7c43-4613-a866-834ca3e6211d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def integrate_pulse(t, y, t_start, t_end):\n",
    "    \"\"\"\n",
    "    Numerical integral of y(t) between t_start and t_end (trapezoidal rule).\n",
    "    For negative pulses on baseline-subtracted data this area will be negative.\n",
    "    \"\"\"\n",
    "    mask = (t >= t_start) & (t <= t_end)\n",
    "    if not np.any(mask):\n",
    "        return 0.0\n",
    "    return np.trapezoid(y[mask], t[mask])  # V * s if y in V and t in s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5178b68c-6050-441b-916a-d676df63b4d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def adaptive_window_negative(t, y, frac_level=0.1):\n",
    "    \"\"\"\n",
    "    Find start/end of a negative pulse where it crosses a fraction of its depth.\n",
    "    frac_level is fraction of |min|, e.g. 0.1 = 10% depth.\n",
    "    \"\"\"\n",
    "    A = y.min()\n",
    "    if A >= 0:\n",
    "        return None, None\n",
    "    thr = frac_level * A  # negative\n",
    "\n",
    "    # indices where pulse is below this threshold\n",
    "    idx = np.where(y <= thr)[0]\n",
    "    if len(idx) == 0:\n",
    "        return None, None\n",
    "\n",
    "    i_start = idx[0]\n",
    "    i_end = idx[-1]\n",
    "    return t[i_start], t[i_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e8d2df-2664-4e2d-8bcb-18a5b3dce812",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def event_charge_negative_adaptive(event_df, channel=\"Ch_2\", cf_frac=0.5, level_frac=0.1, t_pre_max=None):\n",
    "    \"\"\"\n",
    "    Baseline-subtract, locate negative pulse, then integrate over an\n",
    "    amplitude-adaptive window (where |y| > level_frac * |min|).\n",
    "    \"\"\"\n",
    "    t = event_df[\"timestamp\"].to_numpy()\n",
    "    y = event_df[channel].to_numpy()\n",
    "\n",
    "    y_bs, baseline = subtract_baseline(t, y, t_pre_max=t_pre_max)\n",
    "\n",
    "    # ensure a pulse exists\n",
    "    if y_bs.min() >= 0:\n",
    "        return None\n",
    "\n",
    "    # use adaptive window from fraction-of-depth\n",
    "    t_start, t_end = adaptive_window_negative(t, y_bs, frac_level=level_frac)\n",
    "    if t_start is None:\n",
    "        return None\n",
    "\n",
    "    area = integrate_pulse(t, y_bs, t_start, t_end)\n",
    "    return abs(area)  # V·time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c579de-fd6c-4b53-b357-9fe816db6d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_amplitude_spectrum(data, min_thresh=-0.01, bins = 100, channel = 'Ch_2', cf_frac = 0.5, level_frac = 0.1, Run_Number = None, Counts=None, percentage = None):\n",
    "    df = load_df(data)\n",
    "    int_ids = interesting_value_filter(df, min_thresh=min_thresh)\n",
    "    int_df = df[df['event_id'].isin(int_ids)] # keep interesting df\n",
    "\n",
    "    charges = []\n",
    "    for eid in int_df[\"event_id\"].unique():\n",
    "        event = int_df[int_df[\"event_id\"] == eid].sort_values(\"timestamp\")\n",
    "        q = event_charge_negative_adaptive(event, channel=channel, cf_frac = cf_frac, level_frac = level_frac)\n",
    "        if q is not None:\n",
    "            charges.append(q)\n",
    "    \n",
    "    if charges is not None: \n",
    "        print(f'The mean area is: {np.mean(charges)} Vs')\n",
    "        \n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.hist(charges, bins=bins, alpha=0.8, edgecolor=\"k\")\n",
    "        plt.xlabel(f\"Area under Pulse (Vs)\")\n",
    "        plt.ylabel(\"Counts\")\n",
    "        plt.title(f\"Amplitude spectrum of Run {Run_Number} with {Counts} Aquisitions and Ratio {percentage}%\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"AmplitudeSpectrum_Run{Run_Number}_Counts{Counts}.png\", dpi=300)        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9f82a-5c98-4830-b080-f8467274c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_amplitude_spectrum('/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run10.csv',Run_Number='10', Counts='10K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba74acf-1e6e-4912-bb4e-a78bbe057c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_amplitude_spectrum('/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run11.csv', Run_Number='11', Counts = '10K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8adc2c-e845-49e7-82d4-dde1f1e05b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_amplitude_spectrum('/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run12.csv', Run_Number='12', Counts='100K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f723b6-4547-4a20-9565-a872b45b1dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_main(data, bins_fit=1000, Run_Number=None , Counts=None):\n",
    "    percentage = main_ratio_and_plots(data=data, min_thresh=-0.01, show_all_channels=False, show_only_camera=False)\n",
    "    percentage = round(percentage, 3)\n",
    "    \n",
    "    main_jitter_plot_and_fwhm(data=data, bins_fit=bins_fit, Run_Number=Run_Number, Counts=Counts, percentage=percentage)\n",
    "    main_amplitude_spectrum(data=data, Run_Number=Run_Number, Counts=Counts, percentage=percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0ab57-3057-4a9e-a067-840d4e2b346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_main('/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run10.csv' , Run_Number='10', Counts='10K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1233ee-ab98-4912-97c2-de605933160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_main('/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run11.csv' , Run_Number='11', Counts='10K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361646d-1e17-4a9d-a335-ed4fdf7dcaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_main('/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run12.csv' , Run_Number='12', Counts='100K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11ce1e4-2008-48ae-8ab4-48de358e449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### now rewriting the code for Run12 s.t. one can see a more zoomed version of the amplitude spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7216158-bd42-487c-8111-26f9b77f2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_amplitude_spectrum_zoom(data, min_thresh=-0.01, bins = 100, channel = 'Ch_2', cf_frac = 0.5, level_frac = 0.1, Run_Number = None, Counts=None, percentage = None):\n",
    "    df = load_df(data)\n",
    "    int_ids = interesting_value_filter(df, min_thresh=min_thresh)\n",
    "    int_df = df[df['event_id'].isin(int_ids)] # keep interesting df\n",
    "\n",
    "    charges = []\n",
    "    for eid in int_df[\"event_id\"].unique():\n",
    "        event = int_df[int_df[\"event_id\"] == eid].sort_values(\"timestamp\")\n",
    "        q = event_charge_negative_adaptive(event, channel=channel, cf_frac = cf_frac, level_frac = level_frac)\n",
    "        if (q is not None) and (q <= 2e-10):\n",
    "            charges.append(q)\n",
    "    \n",
    "    if charges is not None: \n",
    "        print(f'The mean area is: {np.mean(charges)} Vs')\n",
    "        \n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.hist(charges, bins=bins, alpha=0.8, edgecolor=\"k\")\n",
    "        plt.xlabel(f\"Area under Pulse (Vs)\")\n",
    "        plt.ylabel(\"Counts\")\n",
    "        plt.title(f\"(zoomed) Amplitude spectrum of Run {Run_Number} with {Counts} Aquisitions and Ratio {percentage}%\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"ZoomedAmplitudeSpectrum_Run{Run_Number}_Counts{Counts}.png\", dpi=300)        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29835677-8a1c-4055-872a-27ed28191d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_amplitude_spectrum_zoom('/Users/laloscola/ExternalDrive/magic data laboratory/cleaned_RatioCheck_Run12.csv' , Run_Number='12', Counts='100K', percentage = '4.505')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36faf56-4a44-4e4b-846f-7c447f6c2c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
