











import pandas as pd
import numpy as np
import scipy as sp
import pyarrow.parquet as pq
import time
import matplotlib.pyplot as plt
import argparse
import csv
import pyarrow as pa 


df = pd.read_csv('data_oscill/Multiple_WFM_Run4_list.Wfm.csv', sep = ";")


df.head(10)








RESOLUTION = 4e-11
BUFFER_SIZE = 50_000  # number of rows to buffer before writing

def convert_csv_to_csv_and_parquet(in_path, out_csv_path, out_parquet_path):
    buffer = []
    event_id = -1
    t0 = None
    idx = 0

    parquet_writer = None

    with open(in_path) as fin, open(out_csv_path, "w") as fout:
        # write CSV header
        fout.write("event_id,timestamp,Ch_1,Ch_2,Ch_3\n")

        for line in fin:
            line = line.strip()
            if not line:
                continue

            parts = line.split(";")

            # new event
            if len(parts) == 1:
                event_id += 1
                t0 = float(parts[0])
                idx = 0
                continue

            # data row
            if len(parts) == 3:
                timestamp = t0 + idx * RESOLUTION
                idx += 1
                x, y, z = map(float, parts)

                # add to buffer
                buffer.append([event_id, timestamp, x, y, z])

                # flush buffer if full
                if len(buffer) >= BUFFER_SIZE:
                    # write CSV
                    for row in buffer:
                        fout.write(f"{row[0]},{row[1]},{row[2]},{row[3]},{row[4]}\n")

                    # write Parquet
                    df = pd.DataFrame(buffer, columns=["event_id", "timestamp", "Ch_1", "Ch_2", "Ch_3"])
                    table = pa.Table.from_pandas(df)
                    if parquet_writer is None:
                        parquet_writer = pq.ParquetWriter(out_parquet_path, table.schema, compression='snappy')
                    parquet_writer.write_table(table)

                    buffer = []

                continue

            raise ValueError(f"Unexpected format: {line}")

        # flush remaining buffer
        if buffer:
            for row in buffer:
                fout.write(f"{row[0]},{row[1]},{row[2]},{row[3]},{row[4]}\n")
            df = pd.DataFrame(buffer, columns=["event_id", "timestamp", "Ch_1", "Ch_2", "Ch_3"])
            table = pa.Table.from_pandas(df)
            if parquet_writer is None:
                parquet_writer = pq.ParquetWriter(out_parquet_path, table.schema, compression='snappy')
            parquet_writer.write_table(table)

        if parquet_writer:
            parquet_writer.close()



convert_csv_to_csv_and_parquet('data_oscill/Multiple_WFM_Run4_list.Wfm.csv', 'data_oscill/cleaned_Run4.csv', 'data_oscill/cleaned_Run4.parquet')





def plot_target_event(cleaned_data, target_event = 0):
    df = pd.read_csv(cleaned_data)

    # filter for target event
    event= df[df['event_id'] == target_event] 

    # plot
    plt.figure(figsize=(6,2))
    plt.plot(event["timestamp"], event["Ch_1"], label="Laser Trigger")
    plt.plot(event["timestamp"], event["Ch_2"], label="MCP PMT Camera")
    plt.plot(event["timestamp"], event["Ch_3"], label="HPD")
    plt.xlabel("Timestamp (s)")
    plt.ylabel("Signal")
    plt.title(f"Event {target_event}")
    plt.legend()
    plt.show()


for i in range(0,20):
    plot_target_event('data_oscill/cleaned_Run4.csv', target_event = i)





import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import math

RESOLUTION = 4e-11
BUFFER_SIZE = 50_000

def convert_csv_to_csv_and_parquet_dynamic(in_path, out_csv_path, out_parquet_path):
    buffer = []
    event_id = -1
    t0 = None
    idx = 0
    max_channels = 0
    parquet_writer = None

    with open(in_path) as fin, open(out_csv_path, "w") as fout:
        # CSV header: we will dynamically update later
        header_written = False

        for line in fin:
            line = line.strip()
            if not line:
                continue

            parts = line.split(";")

            # header = new event
            if len(parts) == 1:
                event_id += 1
                t0 = float(parts[0])
                idx = 0
                continue

            # data row = N channels
            if len(parts) >= 2:
                timestamp = t0 + idx * RESOLUTION
                idx += 1
                values = [float(x) for x in parts]

                max_channels = max(max_channels, len(values))

                buffer.append([event_id, timestamp] + values)

                # flush buffer if full
                if len(buffer) >= BUFFER_SIZE:
                    # CSV: pad rows so all have max_channels
                    if not header_written:
                        # write CSV header
                        header = ["event_id", "timestamp"] + [f"value_{i}" for i in range(max_channels)]
                        fout.write(",".join(header) + "\n")
                        header_written = True

                    for row in buffer:
                        row_padded = row + [math.nan]*(max_channels - (len(row)-2))
                        fout.write(",".join(map(str,row_padded)) + "\n")

                    # Parquet
                    df = pd.DataFrame(buffer)
                    if df.shape[1] < max_channels+2:
                        # pad columns
                        for i in range(df.shape[1]-2, max_channels):
                            df[i+2] = math.nan
                    df.columns = ["event_id", "timestamp"] + [f"value_{i}" for i in range(max_channels)]
                    table = pa.Table.from_pandas(df)
                    if parquet_writer is None:
                        parquet_writer = pq.ParquetWriter(out_parquet_path, table.schema, compression='snappy')
                    parquet_writer.write_table(table)

                    buffer = []

                continue

            raise ValueError(f"Unexpected format: {line}")

        # flush remaining buffer
        if buffer:
            if not header_written:
                header = ["event_id", "timestamp"] + [f"value_{i}" for i in range(max_channels)]
                fout.write(",".join(header) + "\n")
            for row in buffer:
                row_padded = row + [math.nan]*(max_channels - (len(row)-2))
                fout.write(",".join(map(str,row_padded)) + "\n")

            df = pd.DataFrame(buffer)
            if df.shape[1] < max_channels+2:
                for i in range(df.shape[1]-2, max_channels):
                    df[i+2] = math.nan
            df.columns = ["event_id", "timestamp"] + [f"value_{i}" for i in range(max_channels)]
            table = pa.Table.from_pandas(df)
            if parquet_writer is None:
                parquet_writer = pq.ParquetWriter(out_parquet_path, table.schema, compression='snappy')
            parquet_writer.write_table(table)

        if parquet_writer:
            parquet_writer.close()




